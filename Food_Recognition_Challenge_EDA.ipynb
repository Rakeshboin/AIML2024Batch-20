{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rakeshboin/AIML2024Batch-20/blob/main/Food_Recognition_Challenge_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'food-model:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F579741%2F1048562%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240425%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240425T035943Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D621253930cf6afd89a7e64f4d0aa040388794e8e8ad857a7f30c9777955ce0f4ff8ac4186bf96fefad58835e9b5d9d0451ee65b1e0d6d78b39afc3936583b1002719c14a77e386359edd792b24cb14fcdedf505e00b50902028a8cf1b46148cfbf6d77d9939afeb091b2b843c617ea73326b7c097f39aa4aaec46933dee7755e461742669b04820f6bc87cf65829bb470bc48225848cc72f4a53b688ba1b15657b03a42d044aa2e5224685001996a4d24f632793b3bf4308528d262b1b298ba632faf5af5d3f07e1ddb1febdac060dca46afab8f9b5a0fd2110304038c3017168606f2652c80c2c7b8ff259adb66da0f165c508f3f9de4c619697701d14bc00c'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "tIR2sDvCaTlx",
        "outputId": "e80c38c8-cb31-46aa-c6a3-73fb55354cbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading food-model, 167191431 bytes compressed\n",
            "[==================================================] 167191431 bytes downloaded\n",
            "Downloaded and uncompressed: food-model\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "metadata": {
        "id": "HWc5kQYSaTl0"
      },
      "cell_type": "markdown",
      "source": [
        "## General information\n",
        "\n",
        "In this kernel I work with a dataset for the Food Recognition Challenge conducted on AIcrowd hosted here: https://www.aicrowd.com/challenges/food-recognition-challenge.\n",
        "```\n",
        "This is a novel dataset of food images collected through the MyFoodRepo app where numerous volunteer Swiss users provide images of their daily food intake in the context of a digital cohort called Food & You. This growing data set has been annotated - or automatic annotations have been verified - with respect to segmentation, classification (mapping the individual food items onto an ontology of Swiss Food items), and weight / volume estimation.\n",
        "\n",
        "This is an evolving dataset, where we will release more data as the dataset grows over time.\n",
        "```\n",
        "\n",
        "![](https://i.imgur.com/Syv1Ycf.png)\n",
        "\n",
        "In this kernel I'll show how to set up environment for this challenge, provide EDA and possible in future show baseline modelling.\n",
        "\n",
        "The code is based on this notebook by organizers: https://colab.research.google.com/drive/1A5p9GX5X3n6OMtLjfhnH6Oeq13tWNtFO#scrollTo=lkjrKJfIVCM3"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "J0271tBjaTl1"
      },
      "cell_type": "markdown",
      "source": [
        "## Setting up environment\n",
        "\n",
        "There are several steps which need to be done:\n",
        "* install certain versions of numpy, tensorflow, keras\n",
        "* clone the Mask_RCNN repo\n",
        "* install requirements and the repo itself\n",
        "* the utils reqires json with annotations to be called `annotation.json`, but we have `annotations.json`; so I copy the whole data and copy this file with a new name"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "id": "3nKdojlwaTl1"
      },
      "cell_type": "code",
      "source": [
        "!rm -rf images assets\n",
        "!pip install numpy==1.17.0\n",
        "!pip install tensorflow==1.15.2\n",
        "!pip install keras==2.1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "5nwWRMayaTl2"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "DATA_DIR = '/kaggle/working/food-recognition-challenge'\n",
        "# Directory to save logs and trained model\n",
        "ROOT_DIR = ''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "id": "5x_qti62aTl2"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://www.github.com/matterport/Mask_RCNN.git\n",
        "os.chdir('Mask_RCNN')\n",
        "!pip install -q -r requirements.txt\n",
        "!python setup.py -q install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "id": "nA_MTa5jaTl2"
      },
      "cell_type": "code",
      "source": [
        "!pip uninstall pycocotools -y\n",
        "!pip install -q git+https://github.com/waleedka/coco.git#subdirectory=PythonAPI"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uAX6XLU9aTl3"
      },
      "cell_type": "markdown",
      "source": [
        "### import libraries"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "lLJIDTZhaTl3"
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(os.path.join('.', 'Mask_RCNN'))  # To find local version of the library\n",
        "sys.path.append(ROOT_DIR)\n",
        "import sys\n",
        "import re\n",
        "import random\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import mrcnn.model as modellib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.lines as lines\n",
        "import matplotlib\n",
        "import math\n",
        "import logging\n",
        "import json\n",
        "import itertools\n",
        "import glob\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools import mask as maskUtils\n",
        "from mrcnn.model import log\n",
        "from mrcnn.config import Config\n",
        "from mrcnn import visualize\n",
        "from mrcnn import utils\n",
        "from matplotlib.patches import Polygon\n",
        "from imgaug import augmenters as iaa\n",
        "from collections import defaultdict, Counter\n",
        "from collections import OrderedDict\n",
        "ROOT_DIR = os.path.abspath(\".\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eaBcFkIEaTl4"
      },
      "cell_type": "markdown",
      "source": [
        "### Defining dataset class and config"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "N-0C95DOaTl4"
      },
      "cell_type": "code",
      "source": [
        "class FoodChallengeDataset(utils.Dataset):\n",
        "    def load_dataset(self, dataset_dir, load_small=False, return_coco=True):\n",
        "        \"\"\" Loads dataset released for the AICrowd Food Challenge\n",
        "            Params:\n",
        "                - dataset_dir : root directory of the dataset (can point to the train/val folder)\n",
        "                - load_small : Boolean value which signals if the annotations for all the images need to be loaded into the memory,\n",
        "                               or if only a small subset of the same should be loaded into memory\n",
        "        \"\"\"\n",
        "        self.load_small = load_small\n",
        "        if self.load_small:\n",
        "            annotation_path = os.path.join(dataset_dir, \"annotation-small.json\")\n",
        "        else:\n",
        "            annotation_path = os.path.join(dataset_dir, \"annotations.json\")\n",
        "\n",
        "        image_dir = os.path.join(dataset_dir, \"images\")\n",
        "        print(\"Annotation Path \", annotation_path)\n",
        "        print(\"Image Dir \", image_dir)\n",
        "        assert os.path.exists(annotation_path) and os.path.exists(image_dir)\n",
        "\n",
        "        self.coco = COCO(annotation_path)\n",
        "        self.image_dir = image_dir\n",
        "\n",
        "        # Load all classes (Only Building in this version)\n",
        "        classIds = self.coco.getCatIds()\n",
        "\n",
        "        # Load all images\n",
        "        image_ids = list(self.coco.imgs.keys())\n",
        "\n",
        "        # register classes\n",
        "        for _class_id in classIds:\n",
        "            self.add_class(\"crowdai-food-challenge\", _class_id, self.coco.loadCats(_class_id)[0][\"name\"])\n",
        "\n",
        "        # Register Images\n",
        "        for _img_id in image_ids:\n",
        "            assert(os.path.exists(os.path.join(image_dir, self.coco.imgs[_img_id]['file_name'])))\n",
        "            self.add_image(\n",
        "                \"crowdai-food-challenge\", image_id=_img_id,\n",
        "                path=os.path.join(image_dir, self.coco.imgs[_img_id]['file_name']),\n",
        "                width=self.coco.imgs[_img_id][\"width\"],\n",
        "                height=self.coco.imgs[_img_id][\"height\"],\n",
        "                annotations=self.coco.loadAnns(self.coco.getAnnIds(\n",
        "                                            imgIds=[_img_id],\n",
        "                                            catIds=classIds,\n",
        "                                            iscrowd=None)))\n",
        "\n",
        "        if return_coco:\n",
        "            return self.coco\n",
        "\n",
        "    def load_mask(self, image_id):\n",
        "        \"\"\" Loads instance mask for a given image\n",
        "              This function converts mask from the coco format to a\n",
        "              a bitmap [height, width, instance]\n",
        "            Params:\n",
        "                - image_id : reference id for a given image\n",
        "\n",
        "            Returns:\n",
        "                masks : A bool array of shape [height, width, instances] with\n",
        "                    one mask per instance\n",
        "                class_ids : a 1D array of classIds of the corresponding instance masks\n",
        "                    (In this version of the challenge it will be of shape [instances] and always be filled with the class-id of the \"Building\" class.)\n",
        "        \"\"\"\n",
        "\n",
        "        image_info = self.image_info[image_id]\n",
        "        assert image_info[\"source\"] == \"crowdai-food-challenge\"\n",
        "\n",
        "        instance_masks = []\n",
        "        class_ids = []\n",
        "        annotations = self.image_info[image_id][\"annotations\"]\n",
        "        # Build mask of shape [height, width, instance_count] and list\n",
        "        # of class IDs that correspond to each channel of the mask.\n",
        "        for annotation in annotations:\n",
        "            class_id = self.map_source_class_id(\n",
        "                \"crowdai-food-challenge.{}\".format(annotation['category_id']))\n",
        "            if class_id:\n",
        "                m = self.annToMask(annotation,  image_info[\"height\"],\n",
        "                                                image_info[\"width\"])\n",
        "                # Some objects are so small that they're less than 1 pixel area\n",
        "                # and end up rounded out. Skip those objects.\n",
        "                if m.max() < 1:\n",
        "                    continue\n",
        "\n",
        "                # Ignore the notion of \"is_crowd\" as specified in the coco format\n",
        "                # as we donot have the said annotation in the current version of the dataset\n",
        "\n",
        "                instance_masks.append(m)\n",
        "                class_ids.append(class_id)\n",
        "        # Pack instance masks into an array\n",
        "        if class_ids:\n",
        "            mask = np.stack(instance_masks, axis=2)\n",
        "            class_ids = np.array(class_ids, dtype=np.int32)\n",
        "            return mask, class_ids\n",
        "        else:\n",
        "            # Call super class to return an empty mask\n",
        "            return super(FoodChallengeDataset, self).load_mask(image_id)\n",
        "\n",
        "\n",
        "    def image_reference(self, image_id):\n",
        "        \"\"\"Return a reference for a particular image\n",
        "\n",
        "            Ideally you this function is supposed to return a URL\n",
        "            but in this case, we will simply return the image_id\n",
        "        \"\"\"\n",
        "        return \"crowdai-food-challenge::{}\".format(image_id)\n",
        "    # The following two functions are from pycocotools with a few changes.\n",
        "\n",
        "    def annToRLE(self, ann, height, width):\n",
        "        \"\"\"\n",
        "        Convert annotation which can be polygons, uncompressed RLE to RLE.\n",
        "        :return: binary mask (numpy 2D array)\n",
        "        \"\"\"\n",
        "        segm = ann['segmentation']\n",
        "        if isinstance(segm, list):\n",
        "            # polygon -- a single object might consist of multiple parts\n",
        "            # we merge all parts into one mask rle code\n",
        "            rles = maskUtils.frPyObjects(segm, height, width)\n",
        "            rle = maskUtils.merge(rles)\n",
        "        elif isinstance(segm['counts'], list):\n",
        "            # uncompressed RLE\n",
        "            rle = maskUtils.frPyObjects(segm, height, width)\n",
        "        else:\n",
        "            # rle\n",
        "            rle = ann['segmentation']\n",
        "        return rle\n",
        "\n",
        "    def annToMask(self, ann, height, width):\n",
        "        \"\"\"\n",
        "        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n",
        "        :return: binary mask (numpy 2D array)\n",
        "        \"\"\"\n",
        "        rle = self.annToRLE(ann, height, width)\n",
        "        m = maskUtils.decode(rle)\n",
        "        return m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BJFTsO0haTl4"
      },
      "cell_type": "markdown",
      "source": [
        "### Warning\n",
        "\n",
        "Please, notice that in config values of `STEPS_PER_EPOCH` and `VALIDATION_STEPS` are quite low. I decreased them so that model would train fast, but the quality will be low. When you train the model, increase the values up to 50-200."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_kg_hide-input": true,
        "id": "gglwSGrxaTl4"
      },
      "cell_type": "code",
      "source": [
        "from mrcnn.config import Config\n",
        "class FoodChallengeConfig(Config):\n",
        "    \"\"\"Configuration for training on data in MS COCO format.\n",
        "    Derives from the base Config class and overrides values specific\n",
        "    to the COCO dataset.\n",
        "    \"\"\"\n",
        "    # Give the configuration a recognizable name\n",
        "    NAME = \"crowdai-food-challenge\"\n",
        "\n",
        "    # We use a GPU with 12GB memory, which can fit two images.\n",
        "    # Adjust down if you use a smaller GPU.\n",
        "    IMAGES_PER_GPU = 2\n",
        "\n",
        "    # Uncomment to train on 8 GPUs (default is 1)\n",
        "    GPU_COUNT = 1\n",
        "    BACKBONE = 'resnet50'\n",
        "    # Number of classes (including background)\n",
        "    NUM_CLASSES = 62  # 1 Background + 61 classes\n",
        "\n",
        "    STEPS_PER_EPOCH=10\n",
        "    VALIDATION_STEPS=10\n",
        "\n",
        "    LEARNING_RATE=0.001\n",
        "    IMAGE_MAX_DIM=256\n",
        "    IMAGE_MIN_DIM=256\n",
        "config = FoodChallengeConfig()\n",
        "config.display()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "YNu7nGMcaTl4"
      },
      "cell_type": "code",
      "source": [
        "%cd ..\n",
        "!cp /kaggle/input/food-recognition-challenge /kaggle/working -r\n",
        "!rm -rf images assets # to prevent displaying images at the bottom of a kernel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "JUiy9vAiaTl5"
      },
      "cell_type": "code",
      "source": [
        "!cp /kaggle/working/food-recognition-challenge/train/train/annotations.json /kaggle/working/food-recognition-challenge/train/train/annotation.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Qr4eBtOaaTl5"
      },
      "cell_type": "code",
      "source": [
        "#from mrcnn.dataset import FoodChallengeDataset\n",
        "dataset_train = FoodChallengeDataset()\n",
        "dataset_train.load_dataset(dataset_dir=os.path.join(\"/kaggle/working/food-recognition-challenge/train\", \"train\"), load_small=False)\n",
        "#dataset_train.load_dataset(dataset_dir=\"train\", load_small=False)\n",
        "dataset_train.prepare()\n",
        "dataset = dataset_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pn8IDbF4aTl5"
      },
      "cell_type": "markdown",
      "source": [
        "## Data exploration"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "kIKqm_vHaTl5"
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "class_counts = Counter()\n",
        "for img_info in dataset_train.image_info:\n",
        "    ann = img_info['annotations']\n",
        "    for i in ann:\n",
        "        class_counts[i['category_id']] += 1\n",
        "class_mapping = {i['id']: i['name'] for i in dataset_train.class_info}\n",
        "\n",
        "class_counts = pd.DataFrame(class_counts.most_common(), columns=['class_name', 'count'])\n",
        "class_counts['class_name'] = class_counts['class_name'].apply(lambda x: class_mapping[x])\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.barh(class_counts['class_name'], class_counts['count'])\n",
        "plt.title('Counts of classes of objects');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "hZ6XRqSVaTl5"
      },
      "cell_type": "code",
      "source": [
        "print(f'We have {class_counts.shape[0]} classes!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hX70k42JaTl6"
      },
      "cell_type": "markdown",
      "source": [
        "The most common is water - I suppose it is a background. Some vegetables and white bread are the most common."
      ]
    },
    {
      "metadata": {
        "id": "XkUjg-YqaTl6"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's use a function from the repo to see information about one random image"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "vyCCdpB8aTl6"
      },
      "cell_type": "code",
      "source": [
        "# Load random image and mask.\n",
        "image_id = random.choice(dataset.image_ids)\n",
        "image = dataset.load_image(image_id)\n",
        "mask, class_ids = dataset.load_mask(image_id)\n",
        "# Compute Bounding box\n",
        "bbox = utils.extract_bboxes(mask)\n",
        "\n",
        "# Display image and additional stats\n",
        "print(\"image_id \", image_id, dataset.image_reference(image_id))\n",
        "log(\"image\", image)\n",
        "log(\"mask\", mask)\n",
        "log(\"class_ids\", class_ids)\n",
        "log(\"bbox\", bbox)\n",
        "# Display image and instances\n",
        "visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names, figsize=(12, 12))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kXvpnsLnaTl6"
      },
      "cell_type": "markdown",
      "source": [
        "Let's see what information we have about images:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "82a4HYHZaTl6"
      },
      "cell_type": "code",
      "source": [
        "dataset_train.image_info[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6Jyn0pr8aTl6"
      },
      "cell_type": "markdown",
      "source": [
        "* There is some meta information: path, height, width\n",
        "* for each object we have annotatioins: class, segmentation mask, total area, bbox coordinates."
      ]
    },
    {
      "metadata": {
        "id": "8KXPyJhTaTl6"
      },
      "cell_type": "markdown",
      "source": [
        "### Masks"
      ]
    },
    {
      "metadata": {
        "id": "WG0Yk99FaTl6"
      },
      "cell_type": "markdown",
      "source": [
        "We can see masks and boxes for different classes. Let's take 10 random classes"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "waLB7-jYaTl7"
      },
      "cell_type": "code",
      "source": [
        "class_images = defaultdict(list)\n",
        "for ind, img_info in enumerate(dataset_train.image_info):\n",
        "    ann = img_info['annotations']\n",
        "    for i in ann:\n",
        "        class_images[i['category_id']].append(ind)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "70_HRf1-aTl7"
      },
      "cell_type": "code",
      "source": [
        "image_ids = np.random.choice(dataset.image_ids, 4)\n",
        "for class_id in np.random.choice(list(class_images.keys()), 10):\n",
        "    image_id = np.random.choice(class_images[class_id], 1)[0]\n",
        "    image = dataset.load_image(image_id)\n",
        "    mask, class_ids = dataset.load_mask(image_id)\n",
        "    visualize.display_top_masks(image, mask, class_ids, dataset.class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C1Jah2UnaTl7"
      },
      "cell_type": "markdown",
      "source": [
        "We can see that some masks are big, some are small. Some have a single area, some have multiple areas."
      ]
    },
    {
      "metadata": {
        "id": "AHgKx5oIaTl7"
      },
      "cell_type": "markdown",
      "source": [
        "### Bounding Boxes"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "eEx1er3VaTl7"
      },
      "cell_type": "code",
      "source": [
        "for idx, class_id in enumerate(np.random.choice(list(class_images.keys()), 10)):\n",
        "    image_id = np.random.choice(class_images[class_id], 1)[0]\n",
        "    image = dataset.load_image(image_id)\n",
        "    mask, class_ids = dataset.load_mask(image_id)\n",
        "    # Compute Bounding box\n",
        "    bbox = utils.extract_bboxes(mask)\n",
        "    visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names, figsize=(8, 8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RQrKxLvmaTl7"
      },
      "cell_type": "markdown",
      "source": [
        "Obviously bounding boxed have masks inside them. And if an object has several masks, then the bounding box will contain all the masks."
      ]
    },
    {
      "metadata": {
        "id": "tbtTic0HaTl7"
      },
      "cell_type": "markdown",
      "source": [
        "### Anchors\n",
        "\n",
        "One more important type of annotatioin is anchor. Anchors are a set of boxes with predefined locations and scales relative to images. These boxes are defined to capture the scale and aspect ratio of specific object classes you want to detect and are typically chosen based on object sizes in your training datasets."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "gf6hPEOSaTl8"
      },
      "cell_type": "code",
      "source": [
        "# Generate Anchors\n",
        "\n",
        "backbone_shapes = modellib.compute_backbone_shapes(config, config.IMAGE_SHAPE)\n",
        "anchors = utils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES,\n",
        "                                          config.RPN_ANCHOR_RATIOS,\n",
        "                                          backbone_shapes,\n",
        "                                          config.BACKBONE_STRIDES,\n",
        "                                          config.RPN_ANCHOR_STRIDE)\n",
        "\n",
        "# Print summary of anchors\n",
        "num_levels = len(backbone_shapes)\n",
        "anchors_per_cell = len(config.RPN_ANCHOR_RATIOS)\n",
        "print(\"Count: \", anchors.shape[0])\n",
        "print(\"Scales: \", config.RPN_ANCHOR_SCALES)\n",
        "print(\"ratios: \", config.RPN_ANCHOR_RATIOS)\n",
        "print(\"Anchors per Cell: \", anchors_per_cell)\n",
        "print(\"Levels: \", num_levels)\n",
        "anchors_per_level = []\n",
        "for l in range(num_levels):\n",
        "    num_cells = backbone_shapes[l][0] * backbone_shapes[l][1]\n",
        "    anchors_per_level.append(anchors_per_cell * num_cells // config.RPN_ANCHOR_STRIDE**2)\n",
        "    print(\"Anchors in Level {}: {}\".format(l, anchors_per_level[l]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "FF3pBvPuaTl8"
      },
      "cell_type": "code",
      "source": [
        "## Visualize anchors of one cell at the center of the feature map of a specific level\n",
        "\n",
        "# Load and draw random image\n",
        "image_id = np.random.choice(dataset.image_ids, 1)[0]\n",
        "image, image_meta, _, _, _ = modellib.load_image_gt(dataset, config, image_id)\n",
        "fig, ax = plt.subplots(1, figsize=(10, 10))\n",
        "ax.imshow(image)\n",
        "levels = len(backbone_shapes)\n",
        "\n",
        "for level in range(levels):\n",
        "    colors = visualize.random_colors(levels)\n",
        "    # Compute the index of the anchors at the center of the image\n",
        "    level_start = sum(anchors_per_level[:level]) # sum of anchors of previous levels\n",
        "    level_anchors = anchors[level_start:level_start+anchors_per_level[level]]\n",
        "    print(\"Level {}. Anchors: {:6}  Feature map Shape: {}\".format(level, level_anchors.shape[0],\n",
        "                                                                  backbone_shapes[level]))\n",
        "    center_cell = backbone_shapes[level] // 2\n",
        "    center_cell_index = (center_cell[0] * backbone_shapes[level][1] + center_cell[1])\n",
        "    level_center = center_cell_index * anchors_per_cell\n",
        "    center_anchor = anchors_per_cell * (\n",
        "        (center_cell[0] * backbone_shapes[level][1] / config.RPN_ANCHOR_STRIDE**2) \\\n",
        "        + center_cell[1] / config.RPN_ANCHOR_STRIDE)\n",
        "    level_center = int(center_anchor)\n",
        "\n",
        "    # Draw anchors. Brightness show the order in the array, dark to bright.\n",
        "    for i, rect in enumerate(level_anchors[level_center:level_center+anchors_per_cell]):\n",
        "        y1, x1, y2, x2 = rect\n",
        "        p = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, facecolor='none',\n",
        "                              edgecolor=(i+1)*np.array(colors[level]) / anchors_per_cell)\n",
        "        ax.add_patch(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "MRw_RT2raTl8"
      },
      "cell_type": "code",
      "source": [
        "# Create data generator\n",
        "random_rois = 2000\n",
        "g = modellib.data_generator(\n",
        "    dataset, config, shuffle=True, random_rois=random_rois,\n",
        "    batch_size=4,\n",
        "    detection_targets=True)\n",
        "# Get Next Image\n",
        "if random_rois:\n",
        "    [normalized_images, image_meta, rpn_match, rpn_bbox, gt_class_ids, gt_boxes, gt_masks, rpn_rois, rois], \\\n",
        "    [mrcnn_class_ids, mrcnn_bbox, mrcnn_mask] = next(g)\n",
        "\n",
        "else:\n",
        "    [normalized_images, image_meta, rpn_match, rpn_bbox, gt_boxes, gt_masks], _ = next(g)\n",
        "\n",
        "image_id = modellib.parse_image_meta(image_meta)[\"image_id\"][0]\n",
        "\n",
        "# Remove the last dim in mrcnn_class_ids. It's only added\n",
        "# to satisfy Keras restriction on target shape.\n",
        "mrcnn_class_ids = mrcnn_class_ids[:,:,0]\n",
        "\n",
        "b = 0\n",
        "\n",
        "# Restore original image (reverse normalization)\n",
        "sample_image = modellib.unmold_image(normalized_images[b], config)\n",
        "\n",
        "# Compute anchor shifts.\n",
        "indices = np.where(rpn_match[b] == 1)[0]\n",
        "refined_anchors = utils.apply_box_deltas(anchors[indices], rpn_bbox[b, :len(indices)] * config.RPN_BBOX_STD_DEV)\n",
        "\n",
        "# Get list of positive anchors\n",
        "positive_anchor_ids = np.where(rpn_match[b] == 1)[0]\n",
        "negative_anchor_ids = np.where(rpn_match[b] == -1)[0]\n",
        "neutral_anchor_ids = np.where(rpn_match[b] == 0)[0]\n",
        "\n",
        "# ROI breakdown by class\n",
        "for c, n in zip(dataset.class_names, np.bincount(mrcnn_class_ids[b].flatten())):\n",
        "    if n:\n",
        "        print(\"{:23}: {}\".format(c[:20], n))\n",
        "\n",
        "# Show positive anchors\n",
        "visualize.draw_boxes(sample_image, boxes=anchors[positive_anchor_ids],\n",
        "                     refined_boxes=refined_anchors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F8HiAJrnaTl8"
      },
      "cell_type": "markdown",
      "source": [
        "### ROI\n",
        "Region of interest pooling (also known as RoI pooling) is an operation widely used in object detection tasks using convolutional neural networks. For example, to detect multiple fruits and vegetables in a single image. Its purpose is to perform max pooling on inputs of nonuniform sizes to obtain fixed-size feature maps (e.g. 7×7).\n",
        "\n",
        "The result is that from a list of rectangles with different sizes we can quickly get a list of corresponding feature maps with a fixed size. Note that the dimension of the RoI pooling output doesn’t actually depend on the size of the input feature map nor on the size of the region proposals. It’s determined solely by the number of sections we divide the proposal into. What’s the benefit of RoI pooling? One of them is processing speed. If there are multiple object proposals on the frame (and usually there’ll be a lot of them), we can still use the same input feature map for all of them. Since computing the convolutions at early stages of processing is very expensive, this approach can save us a lot of time."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "eT32fdCVaTl8"
      },
      "cell_type": "code",
      "source": [
        "if random_rois:\n",
        "    # Class aware bboxes\n",
        "    bbox_specific = mrcnn_bbox[b, np.arange(mrcnn_bbox.shape[1]), mrcnn_class_ids[b], :]\n",
        "\n",
        "    # Refined ROIs\n",
        "    refined_rois = utils.apply_box_deltas(rois[b].astype(np.float32), bbox_specific[:,:4] * config.BBOX_STD_DEV)\n",
        "\n",
        "    # Class aware masks\n",
        "    mask_specific = mrcnn_mask[b, np.arange(mrcnn_mask.shape[1]), :, :, mrcnn_class_ids[b]]\n",
        "\n",
        "    visualize.draw_rois(sample_image, rois[b], refined_rois, mask_specific, mrcnn_class_ids[b], dataset.class_names)\n",
        "\n",
        "    # Any repeated ROIs?\n",
        "    rows = np.ascontiguousarray(rois[b]).view(np.dtype((np.void, rois.dtype.itemsize * rois.shape[-1])))\n",
        "    _, idx = np.unique(rows, return_index=True)\n",
        "    print(\"Unique ROIs: {} out of {}\".format(len(idx), rois.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uize56R1aTmA"
      },
      "cell_type": "markdown",
      "source": [
        "## Modelling with Mask-RCNN\n",
        "\n",
        "The code is based on the baseling by organizers:\n",
        "https://discourse.aicrowd.com/t/new-starter-notebook-paperspace/2754/1"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "g8XXv82iaTmA"
      },
      "cell_type": "code",
      "source": [
        "!mkdir pretrained"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "iszktFqqaTmA"
      },
      "cell_type": "code",
      "source": [
        "PRETRAINED_MODEL_PATH = os.path.join(\"pretrained\", \"mask_rcnn_coco.h5\")\n",
        "LOGS_DIRECTORY = os.path.join(ROOT_DIR, \"logs\")\n",
        "if not os.path.exists(PRETRAINED_MODEL_PATH):\n",
        "    utils.download_trained_weights(PRETRAINED_MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "W4lkDimAaTmA"
      },
      "cell_type": "code",
      "source": [
        "import keras.backend\n",
        "K = keras.backend.backend()\n",
        "if K=='tensorflow':\n",
        "    keras.backend.common.image_dim_ordering()\n",
        "model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=LOGS_DIRECTORY)\n",
        "model_path = PRETRAINED_MODEL_PATH\n",
        "model.load_weights(model_path, by_name=True, exclude=[\n",
        "        \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n",
        "        \"mrcnn_bbox\", \"mrcnn_mask\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "7HTauZ0UaTmA"
      },
      "cell_type": "code",
      "source": [
        "dataset_train = FoodChallengeDataset()\n",
        "dataset_train.load_dataset(os.path.join(\"/kaggle/working/food-recognition-challenge/train\", \"train\"), load_small=False)\n",
        "dataset_train.prepare()\n",
        "dataset_val = FoodChallengeDataset()\n",
        "val_coco = dataset_val.load_dataset(dataset_dir=os.path.join(\"/kaggle/working/food-recognition-challenge/val\", \"val\"), load_small=False, return_coco=True)\n",
        "dataset_val.prepare()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "OZQolNP4aTmB"
      },
      "cell_type": "code",
      "source": [
        "class_names = dataset_train.class_names\n",
        "assert len(class_names)==62, \"Please check DatasetConfig\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "_5M8_4w-aTmB"
      },
      "cell_type": "code",
      "source": [
        "print(\"Training network\")\n",
        "model.train(dataset_train, dataset_val,\n",
        "            learning_rate=config.LEARNING_RATE,\n",
        "            epochs=1,\n",
        "            layers='heads')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3-ZTyhzJaTmB"
      },
      "cell_type": "markdown",
      "source": [
        "## Looking at the predictions"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "lcDRG84AaTmB"
      },
      "cell_type": "code",
      "source": [
        "model_path = model.find_last()\n",
        "model_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "yczJUPonaTmB"
      },
      "cell_type": "code",
      "source": [
        "# I'll use my model trained locally\n",
        "model_path = '/kaggle/input/food-model/mask_rcnn_crowdai-food-challenge_0010.h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "AlIkAk7aaTmB"
      },
      "cell_type": "code",
      "source": [
        "class InferenceConfig(FoodChallengeConfig):\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "    NUM_CLASSES = 62  # 1 Background + 61 classes\n",
        "    IMAGE_MAX_DIM=256\n",
        "    IMAGE_MIN_DIM=256\n",
        "    NAME = \"food\"\n",
        "    DETECTION_MIN_CONFIDENCE=0\n",
        "\n",
        "inference_config = InferenceConfig()\n",
        "inference_config.display()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "9p2nluh9aTmB"
      },
      "cell_type": "code",
      "source": [
        "# Recreate the model in inference mode\n",
        "model = modellib.MaskRCNN(mode='inference',\n",
        "                          config=inference_config,\n",
        "                          model_dir=ROOT_DIR)\n",
        "\n",
        "# Load trained weights (fill in path to trained weights here)\n",
        "assert model_path != \"\", \"Provide path to trained weights\"\n",
        "print(\"Loading weights from \", model_path)\n",
        "model.load_weights(model_path, by_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Qor0yqvBaTmB"
      },
      "cell_type": "code",
      "source": [
        "# Show few example of ground truth vs. predictions on the validation dataset\n",
        "dataset = dataset_val\n",
        "fig = plt.figure(figsize=(10, 30))\n",
        "\n",
        "for i in range(4):\n",
        "\n",
        "    image_id = random.choice(dataset.image_ids)\n",
        "\n",
        "    original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "        modellib.load_image_gt(dataset_val, inference_config,\n",
        "                               image_id, use_mini_mask=False)\n",
        "\n",
        "    print(original_image.shape)\n",
        "    plt.subplot(6, 2, 2*i + 1)\n",
        "    visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id,\n",
        "                                dataset.class_names, ax=fig.axes[-1])\n",
        "\n",
        "    plt.subplot(6, 2, 2*i + 2)\n",
        "    results = model.detect([original_image]) #, verbose=1)\n",
        "    r = results[0]\n",
        "    visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'],\n",
        "                                dataset.class_names, r['scores'], ax=fig.axes[-1])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Food Recognition Challenge EDA",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}